from langchain_community.llms import Ollama
from typing import List, Dict, Any, AsyncGenerator
from dotenv import load_dotenv
import os

load_dotenv() 

llm_model = os.getenv("LLM_MODEL_NAME")
llm_temp=os.getenv("LLM_TEMPERATURE")
def get_llm(model_name=llm_model, temperature=llm_temp):
    """Initializes and returns the LangChain LLM object."""
    return Ollama(model=model_name, temperature=temperature)

async def explain_data_stream(result: List[Dict[str, Any]], question: str) -> AsyncGenerator[str, None]:
    """
    Analyzes data and streams the explanation from the LLM.

    Args:
        result: The data returned from the SQL query.
        question: The user's original question.

    Yields:
        Chunks of the explanation text as they are generated by the LLM.
    """
    if not result:
        yield "The query returned no results."
        return

    sample_rows = result[:3]  
    llm = get_llm()

    prompt = f"""
You are an expert data analyst. A user asked the following question: "{question}"
The data query returned these sample rows:
{sample_rows}

Based on this, provide a concise, insightful explanation in plain English.
Do not repeat the question or the data. Just provide the analysis.
"""
    async for chunk in llm.astream(prompt):
        yield chunk